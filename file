import requests
from bs4 import BeautifulSoup
import json
import concurrent.futures



with open('links.json', 'r') as data_file:
   data = json.load(data_file)


urls = []
for links in data:
   urls.append(links['Links'])




proxies = ['socks5:185.153.198.223:25904','socks5:185.153.198.226:64861','socks5:185.153.198.210:42769','socks5:178.32.213.119:25730','socks5:185.153.198.223:13994','socks5:185.153.198.223:34731','socks5:185.153.198.210:47788','socks5:178.32.213.127:26894','185.153.198.231:45537','178.32.213.127:48962']







def findDomain(proxy):
   for url in urls:
      print(url)
      request = requests.get(url, proxies={'http': proxy, 'https': proxy}, timeout=3)
      soup = BeautifulSoup(request.content, 'html.parser')
      links = soup.find('a', class_='curator_url ttip')
      if links is None:
         pass
      else:
         links = links.get('href')
         if 'https://steamcommunity.com/linkfilter/?url=' in links:
            links = links.split('=')[1]
            data.append({'Domain': links})
         else:
            data.append({'Domain': links})


for proxy in proxies:
   findDomain(proxy)    # WORK

# with concurrent.futures.ThreadPoolExecutor() as executor:    #DIDN't WORK
#    executor.map(findDomain, proxies)      


with open('data.json', 'w') as file:
   json.dump(data, file, indent=4, ensure_ascii=False)
